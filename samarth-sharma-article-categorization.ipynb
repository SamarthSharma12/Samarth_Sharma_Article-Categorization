{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Data Overview"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T04:53:47.546452Z","iopub.status.busy":"2024-10-13T04:53:47.546062Z","iopub.status.idle":"2024-10-13T04:53:48.505885Z","shell.execute_reply":"2024-10-13T04:53:48.504913Z","shell.execute_reply.started":"2024-10-13T04:53:47.546414Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Shape: (4000, 2)\n","First 5 rows of the dataset:\n","                                             Article  Category\n","0  Sudan Govt rejects call to separate religion, ...         0\n","1  Hassan:  #39;Abhorrent act #39; says Blair Wes...         0\n","2  Sharon Says Gaza Evacuation Set for 2005 (AP) ...         0\n","3  Prince Charles chastised for  quot;old fashion...         0\n","4  U.S. Says N.Korea Blast Probably Not Nuclear  ...         0\n","Dataset Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 4000 entries, 0 to 3999\n","Data columns (total 2 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   Article   4000 non-null   object\n"," 1   Category  4000 non-null   int64 \n","dtypes: int64(1), object(1)\n","memory usage: 62.6+ KB\n","None\n","Missing Values:\n"," Article     0\n","Category    0\n","dtype: int64\n"]}],"source":["\n","import pandas as pd\n","\n","data = pd.read_csv('/kaggle/input/article-categorization-dataset/article_data.csv')\n","\n","print(\"Dataset Shape:\", data.shape)\n","\n","# Show the first few rows of the dataset\n","print(\"First 5 rows of the dataset:\")\n","print(data.head())\n","\n","# Print an overview of the dataset (data types and missing values)\n","print(\"Dataset Info:\")\n","print(data.info())\n","\n","# Check for missing values\n","missing_values = data.isnull().sum()\n","print(\"Missing Values:\\n\", missing_values)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model Building - Sentence Transformer + Machine Learning"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T15:51:41.276360Z","iopub.status.busy":"2024-10-12T15:51:41.275597Z","iopub.status.idle":"2024-10-12T15:51:54.671271Z","shell.execute_reply":"2024-10-12T15:51:54.669958Z","shell.execute_reply.started":"2024-10-12T15:51:41.276321Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting sentence-transformers\n","  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\n","Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\n","Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: sentence-transformers\n","Successfully installed sentence-transformers-3.2.0\n"]}],"source":["!pip install sentence-transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T15:51:54.674133Z","iopub.status.busy":"2024-10-12T15:51:54.673732Z","iopub.status.idle":"2024-10-12T16:01:53.487965Z","shell.execute_reply":"2024-10-12T16:01:53.486906Z","shell.execute_reply.started":"2024-10-12T15:51:54.674092Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm, trange\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c116185e6d184cb9af366a46d0f0b28e","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"03a386c6dd7c40b09afc2454eb48bbcf","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ca72787945a49d48ec34311b1239762","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e1d4181b7494914814e8d0d81bb93d6","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3688f7578ad24548bd5c126d591dcad7","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8139afa0b8d9411aa0727f0c27419579","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebe1852de7484c8bb045a45217f9f028","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"475d2b8170f148718e98cb59708e145b","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb6b8600aa1c43c28fe1b1cc0cc245b7","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b329375e3bc4fb09eb4efcf7c09e5bd","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bae3afbeec6f444aa51d34ffa4efdc97","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8cdb9c29c50468cb080bf0b907399f3","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/125 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Base Model Performance:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.87      0.90       209\n","           1       0.94      0.96      0.95       213\n","           2       0.80      0.82      0.81       194\n","           3       0.84      0.85      0.85       184\n","\n","    accuracy                           0.88       800\n","   macro avg       0.88      0.88      0.88       800\n","weighted avg       0.88      0.88      0.88       800\n","\n","Class-Weighted Model Performance:\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.85      0.88       209\n","           1       0.94      0.96      0.95       213\n","           2       0.82      0.85      0.84       194\n","           3       0.83      0.83      0.83       184\n","\n","    accuracy                           0.88       800\n","   macro avg       0.87      0.87      0.87       800\n","weighted avg       0.88      0.88      0.88       800\n","\n","Best Random Forest Model Performance:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.86      0.89       209\n","           1       0.94      0.98      0.96       213\n","           2       0.84      0.83      0.83       194\n","           3       0.80      0.84      0.82       184\n","\n","    accuracy                           0.88       800\n","   macro avg       0.88      0.88      0.88       800\n","weighted avg       0.88      0.88      0.88       800\n","\n","Best Parameters: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n"]}],"source":["from sentence_transformers import SentenceTransformer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Load sentence transformer model\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Encode the dataset (Article is the text column, and Category is the label)\n","X = model.encode(data['Article'].values)\n","y = data['Category'].values\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Random Forest Base Model\n","clf = RandomForestClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate the base model\n","print(\"Base Model Performance:\")\n","print(classification_report(y_test, y_pred))\n","\n","# Random Forest with Class Weights\n","clf_weighted = RandomForestClassifier(class_weight='balanced', random_state=42)\n","clf_weighted.fit(X_train, y_train)\n","y_pred_weighted = clf_weighted.predict(X_test)\n","\n","# Evaluate the class_weight model\n","print(\"Class-Weighted Model Performance:\")\n","print(classification_report(y_test, y_pred_weighted))\n","\n","# Hyperparameter Tuning using GridSearchCV\n","param_grid = {\n","    'n_estimators': [100, 200],\n","    'max_depth': [10, 20],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 2, 4]\n","}\n","\n","grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n","grid_search.fit(X_train, y_train)\n","\n","# Best hyperparameters and model performance\n","best_rf = grid_search.best_estimator_\n","y_pred_best = best_rf.predict(X_test)\n","print(\"Best Random Forest Model Performance:\")\n","print(classification_report(y_test, y_pred_best))\n","print(\"Best Parameters:\", grid_search.best_params_)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Model Building - Transformer"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T16:20:01.546495Z","iopub.status.busy":"2024-10-12T16:20:01.546084Z","iopub.status.idle":"2024-10-12T16:30:15.212512Z","shell.execute_reply":"2024-10-12T16:30:15.211439Z","shell.execute_reply.started":"2024-10-12T16:20:01.546455Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1200/1200 09:58, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.336400</td>\n","      <td>0.371815</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.336700</td>\n","      <td>0.328879</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.266700</td>\n","      <td>0.367618</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Evaluation Results: {'eval_loss': 0.36761805415153503, 'eval_runtime': 13.5442, 'eval_samples_per_second': 59.066, 'eval_steps_per_second': 7.383, 'epoch': 3.0}\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset\n","\n","# Load the BERT tokenizer and dataset\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","train_texts, test_texts, train_labels, test_labels = train_test_split(\n","    data['Article'].tolist(), data['Category'].tolist(), test_size=0.2, random_state=42\n",")\n","\n","\n","class TextDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=512):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokenized_input = self.tokenizer(\n","            self.texts[idx], padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n","        )\n","        return {\n","            'input_ids': tokenized_input['input_ids'].squeeze(0),\n","            'attention_mask': tokenized_input['attention_mask'].squeeze(0),\n","            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","\n","\n","train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n","test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n","\n","\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(train_labels)))\n","\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    evaluation_strategy='epoch',\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=3,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    report_to=\"none\" \n",")\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","evaluation_results = trainer.evaluate()\n","print(\"Evaluation Results:\", evaluation_results)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Model Performance Comparison and Final Model Selection"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-12T16:54:16.335641Z","iopub.status.busy":"2024-10-12T16:54:16.335019Z","iopub.status.idle":"2024-10-12T16:54:30.226950Z","shell.execute_reply":"2024-10-12T16:54:30.225901Z","shell.execute_reply.started":"2024-10-12T16:54:16.335590Z"},"trusted":true},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Random Forest Accuracy: 0.87875\n","Transformer Accuracy: 0.92\n","Transformer selected as the best model.\n"]}],"source":["import torch\n","from sklearn.metrics import accuracy_score\n","from transformers import BertTokenizer\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","if isinstance(final_model, torch.nn.Module):\n","    final_model.to(device)\n","\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","new_test_data = ['Sample text for classification']  \n","\n","\n","if isinstance(final_model, RandomForestClassifier):\n","   \n","    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  \n","    X_new_test = sentence_model.encode(new_test_data)  \n","    predictions = final_model.predict(X_new_test)  \n","\n","else:\n","    \n","    tokenized_test = tokenizer(new_test_data, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","    tokenized_test = {key: val.to(device) for key, val in tokenized_test.items()} \n","\n","   \n","    with torch.no_grad():\n","        logits = final_model(**tokenized_test).logits\n","        predictions = torch.argmax(logits, dim=1).cpu().numpy()  \n","\n","\n","\n","\n","rf_best_accuracy = accuracy_score(y_test, y_pred_best)\n","\n","\n","transformer_predictions = trainer.predict(test_dataset).predictions\n","transformer_pred_labels = torch.argmax(torch.tensor(transformer_predictions), dim=1).numpy()\n","transformer_accuracy = accuracy_score(test_labels, transformer_pred_labels)\n","\n","# Print performance comparison\n","print(f\"Random Forest Accuracy: {rf_best_accuracy}\")\n","print(f\"Transformer Accuracy: {transformer_accuracy}\")\n","\n","# Select the best model based on accuracy\n","if rf_best_accuracy > transformer_accuracy:\n","    print(\"Random Forest selected as the best model.\")\n","    final_model = best_rf\n","else:\n","    print(\"Transformer selected as the best model.\")\n","    final_model = model\n","\n","\n","if isinstance(final_model, RandomForestClassifier):\n","    \n","    X_new_test = sentence_model.encode(new_test_data)\n","    predictions = final_model.predict(X_new_test)\n","else:\n","    \n","    tokenized_test = tokenizer(new_test_data, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","    tokenized_test = {key: val.to(device) for key, val in tokenized_test.items()}\n","\n","    with torch.no_grad():\n","        logits = final_model(**tokenized_test).logits\n","        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Actionable Insights and Recommendations\n"]},{"cell_type":"markdown","metadata":{},"source":["**Model Performance:**\n","\n","* Transformer Accuracy: 92%\n","* Random Forest Accuracy: 87.88%\n","* Preferred Model: Transformer selected for article classification.\n","\n","**Model Deployment:**\n","\n","* Deploy the Transformer model in a production environment.\n","* Create a user-friendly interface or API for easy article submission and classification.\n","\n","**Continuous Monitoring and Retraining:**\n","\n","* Establish a system for regular performance monitoring.\n","* Retrain the model with new data periodically to maintain accuracy.\n","\n","**Model Interpretability:**\n","\n","* Utilize SHAP or LIME for feature importance analysis.\n","* Display confidence scores alongside predictions to enhance user trust.\n","\n","**Model Optimization:**\n","\n","* Conduct hyperparameter tuning to enhance model performance.\n","* Experiment with different Transformer architectures, such as DistilBERT or RoBERTa.\n","\n","**Evaluation Metrics:**\n","\n","* Use additional evaluation metrics like F1-score, precision, and recall for balanced assessment.\n","* Implement a confusion matrix to visualize performance across different classes."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5863192,"sourceId":9609174,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"}},"nbformat":4,"nbformat_minor":4}
